{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "from keras.layers.core import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(input_dim=100, output_dim=512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(layers.Dense(128*49))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(layers.Reshape((7, 7, 128), input_shape=(128*49,)))\n",
    "    model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "    model.add(layers.Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "    model.add(layers.Conv2D(1, (3, 3), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(5,5),padding='same',input_shape=(28,28,1))) #mnist image dimensions (28*28*1)\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(64,(5,5)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(128,(3,3)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_combined(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Fake model for loss function ########\n",
    "def my_loss_model(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable= False # the loss of discriminator will be passed on to the generator\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Training ################################\n",
    "BATCH_SIZE=100\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train = X_train[:, :, :, None]\n",
    "X_test = X_test[:, :, :, None]\n",
    "\n",
    "discriminator = my_discriminator()\n",
    "generator = my_generator()\n",
    "loss_3 = my_loss_model(generator,discriminator)\n",
    "disc_op = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "gen_op = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "loss_3.compile(loss='binary_crossentropy', optimizer=gen_op)\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=disc_op)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch is\", epoch)\n",
    "    print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "    for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "        noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "        image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "        generated_images = generator.predict(noise, verbose=0)\n",
    "        if index % 20 == 0:\n",
    "            image = images_combined(generated_images)\n",
    "            image = image*127.5+127.5\n",
    "            Image.fromarray(image.astype(np.uint8)).save(\n",
    "                str(epoch)+\"_\"+str(index)+\".png\")\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "        d_loss = discriminator.train_on_batch(X, y)\n",
    "        print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        discriminator.trainable = False\n",
    "        g_loss = loss_3.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "        discriminator.trainable = True\n",
    "        print(\"batch %d My mixed loss is : %f\" % (index, g_loss))\n",
    "        if index % 10 == 9:\n",
    "            generator.save_weights('generator', True)\n",
    "            discriminator.save_weights('discriminator', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice=False):\n",
    "    generator = my_generator()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    generator.load_weights('generator')\n",
    "    if nice:\n",
    "        discriminator = my_discriminator()\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        discriminator.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        d_pret = discriminator.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
